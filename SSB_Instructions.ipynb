{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SpaceSaverBERT Instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook explains how to use SpaceSaverBERT to reduce the space needed to store HuggingFace NLP models. After setting up, it walks through saving space with `save_space` and `save_space_opt`, which save space with different mechanisms. Then, it walks through regenerating the reduced-space parameters with `generate_list` and `generate_list_opt`, depending on which save-space function was used.\n",
    "\n",
    "To only use the force-save space method, follow the directions under **Force-Save Space** and **Generating Force-Saved Model from Reduced Model.**\n",
    "\n",
    "To only use the optionally-save space method, follow the directions under **Optionally-Save Space** and **Generate Optionally-Saved Model from Optionally-Reduced Model.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Up\n",
    "\n",
    "Import the following packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import dask\n",
    "import dask.array as da\n",
    "import time\n",
    "import SpaceSaverBERT as ssb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reducing Model Size\n",
    "\n",
    "This demonstrates how to import a model, reduce the size of some stored parameters for more efficient download and upload, and save it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Model\n",
    "\n",
    "For this demonstration, I am loading a BERT model that is 1.04 GB in size. It is downloaded from HuggingFace and called 'Giannipinelli-xlm-roberta-base-finetuned-marc-en'. Load your selected model like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load('pretrained_models/Giannipinelli-xlm-roberta-base-finetuned-marc-en/pytorch_model.bin', map_location='cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pick Parameters to Reduce\n",
    "\n",
    "View the parameters and decide which type you want to reduce the size of."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['roberta.embeddings.position_ids',\n",
       " 'roberta.embeddings.word_embeddings.weight',\n",
       " 'roberta.embeddings.position_embeddings.weight',\n",
       " 'roberta.embeddings.token_type_embeddings.weight',\n",
       " 'roberta.embeddings.LayerNorm.weight',\n",
       " 'roberta.embeddings.LayerNorm.bias',\n",
       " 'roberta.encoder.layer.0.attention.self.query.weight',\n",
       " 'roberta.encoder.layer.0.attention.self.query.bias',\n",
       " 'roberta.encoder.layer.0.attention.self.key.weight',\n",
       " 'roberta.encoder.layer.0.attention.self.key.bias',\n",
       " 'roberta.encoder.layer.0.attention.self.value.weight',\n",
       " 'roberta.encoder.layer.0.attention.self.value.bias',\n",
       " 'roberta.encoder.layer.0.attention.output.dense.weight',\n",
       " 'roberta.encoder.layer.0.attention.output.dense.bias',\n",
       " 'roberta.encoder.layer.0.attention.output.LayerNorm.weight',\n",
       " 'roberta.encoder.layer.0.attention.output.LayerNorm.bias',\n",
       " 'roberta.encoder.layer.0.intermediate.dense.weight',\n",
       " 'roberta.encoder.layer.0.intermediate.dense.bias',\n",
       " 'roberta.encoder.layer.0.output.dense.weight',\n",
       " 'roberta.encoder.layer.0.output.dense.bias',\n",
       " 'roberta.encoder.layer.0.output.LayerNorm.weight',\n",
       " 'roberta.encoder.layer.0.output.LayerNorm.bias',\n",
       " 'roberta.encoder.layer.1.attention.self.query.weight',\n",
       " 'roberta.encoder.layer.1.attention.self.query.bias',\n",
       " 'roberta.encoder.layer.1.attention.self.key.weight',\n",
       " 'roberta.encoder.layer.1.attention.self.key.bias',\n",
       " 'roberta.encoder.layer.1.attention.self.value.weight',\n",
       " 'roberta.encoder.layer.1.attention.self.value.bias',\n",
       " 'roberta.encoder.layer.1.attention.output.dense.weight',\n",
       " 'roberta.encoder.layer.1.attention.output.dense.bias',\n",
       " 'roberta.encoder.layer.1.attention.output.LayerNorm.weight',\n",
       " 'roberta.encoder.layer.1.attention.output.LayerNorm.bias',\n",
       " 'roberta.encoder.layer.1.intermediate.dense.weight',\n",
       " 'roberta.encoder.layer.1.intermediate.dense.bias',\n",
       " 'roberta.encoder.layer.1.output.dense.weight',\n",
       " 'roberta.encoder.layer.1.output.dense.bias',\n",
       " 'roberta.encoder.layer.1.output.LayerNorm.weight',\n",
       " 'roberta.encoder.layer.1.output.LayerNorm.bias',\n",
       " 'roberta.encoder.layer.2.attention.self.query.weight',\n",
       " 'roberta.encoder.layer.2.attention.self.query.bias',\n",
       " 'roberta.encoder.layer.2.attention.self.key.weight',\n",
       " 'roberta.encoder.layer.2.attention.self.key.bias',\n",
       " 'roberta.encoder.layer.2.attention.self.value.weight',\n",
       " 'roberta.encoder.layer.2.attention.self.value.bias',\n",
       " 'roberta.encoder.layer.2.attention.output.dense.weight',\n",
       " 'roberta.encoder.layer.2.attention.output.dense.bias',\n",
       " 'roberta.encoder.layer.2.attention.output.LayerNorm.weight',\n",
       " 'roberta.encoder.layer.2.attention.output.LayerNorm.bias',\n",
       " 'roberta.encoder.layer.2.intermediate.dense.weight',\n",
       " 'roberta.encoder.layer.2.intermediate.dense.bias',\n",
       " 'roberta.encoder.layer.2.output.dense.weight',\n",
       " 'roberta.encoder.layer.2.output.dense.bias',\n",
       " 'roberta.encoder.layer.2.output.LayerNorm.weight',\n",
       " 'roberta.encoder.layer.2.output.LayerNorm.bias',\n",
       " 'roberta.encoder.layer.3.attention.self.query.weight',\n",
       " 'roberta.encoder.layer.3.attention.self.query.bias',\n",
       " 'roberta.encoder.layer.3.attention.self.key.weight',\n",
       " 'roberta.encoder.layer.3.attention.self.key.bias',\n",
       " 'roberta.encoder.layer.3.attention.self.value.weight',\n",
       " 'roberta.encoder.layer.3.attention.self.value.bias',\n",
       " 'roberta.encoder.layer.3.attention.output.dense.weight',\n",
       " 'roberta.encoder.layer.3.attention.output.dense.bias',\n",
       " 'roberta.encoder.layer.3.attention.output.LayerNorm.weight',\n",
       " 'roberta.encoder.layer.3.attention.output.LayerNorm.bias',\n",
       " 'roberta.encoder.layer.3.intermediate.dense.weight',\n",
       " 'roberta.encoder.layer.3.intermediate.dense.bias',\n",
       " 'roberta.encoder.layer.3.output.dense.weight',\n",
       " 'roberta.encoder.layer.3.output.dense.bias',\n",
       " 'roberta.encoder.layer.3.output.LayerNorm.weight',\n",
       " 'roberta.encoder.layer.3.output.LayerNorm.bias',\n",
       " 'roberta.encoder.layer.4.attention.self.query.weight',\n",
       " 'roberta.encoder.layer.4.attention.self.query.bias',\n",
       " 'roberta.encoder.layer.4.attention.self.key.weight',\n",
       " 'roberta.encoder.layer.4.attention.self.key.bias',\n",
       " 'roberta.encoder.layer.4.attention.self.value.weight',\n",
       " 'roberta.encoder.layer.4.attention.self.value.bias',\n",
       " 'roberta.encoder.layer.4.attention.output.dense.weight',\n",
       " 'roberta.encoder.layer.4.attention.output.dense.bias',\n",
       " 'roberta.encoder.layer.4.attention.output.LayerNorm.weight',\n",
       " 'roberta.encoder.layer.4.attention.output.LayerNorm.bias',\n",
       " 'roberta.encoder.layer.4.intermediate.dense.weight',\n",
       " 'roberta.encoder.layer.4.intermediate.dense.bias',\n",
       " 'roberta.encoder.layer.4.output.dense.weight',\n",
       " 'roberta.encoder.layer.4.output.dense.bias',\n",
       " 'roberta.encoder.layer.4.output.LayerNorm.weight',\n",
       " 'roberta.encoder.layer.4.output.LayerNorm.bias',\n",
       " 'roberta.encoder.layer.5.attention.self.query.weight',\n",
       " 'roberta.encoder.layer.5.attention.self.query.bias',\n",
       " 'roberta.encoder.layer.5.attention.self.key.weight',\n",
       " 'roberta.encoder.layer.5.attention.self.key.bias',\n",
       " 'roberta.encoder.layer.5.attention.self.value.weight',\n",
       " 'roberta.encoder.layer.5.attention.self.value.bias',\n",
       " 'roberta.encoder.layer.5.attention.output.dense.weight',\n",
       " 'roberta.encoder.layer.5.attention.output.dense.bias',\n",
       " 'roberta.encoder.layer.5.attention.output.LayerNorm.weight',\n",
       " 'roberta.encoder.layer.5.attention.output.LayerNorm.bias',\n",
       " 'roberta.encoder.layer.5.intermediate.dense.weight',\n",
       " 'roberta.encoder.layer.5.intermediate.dense.bias',\n",
       " 'roberta.encoder.layer.5.output.dense.weight',\n",
       " 'roberta.encoder.layer.5.output.dense.bias',\n",
       " 'roberta.encoder.layer.5.output.LayerNorm.weight',\n",
       " 'roberta.encoder.layer.5.output.LayerNorm.bias',\n",
       " 'roberta.encoder.layer.6.attention.self.query.weight',\n",
       " 'roberta.encoder.layer.6.attention.self.query.bias',\n",
       " 'roberta.encoder.layer.6.attention.self.key.weight',\n",
       " 'roberta.encoder.layer.6.attention.self.key.bias',\n",
       " 'roberta.encoder.layer.6.attention.self.value.weight',\n",
       " 'roberta.encoder.layer.6.attention.self.value.bias',\n",
       " 'roberta.encoder.layer.6.attention.output.dense.weight',\n",
       " 'roberta.encoder.layer.6.attention.output.dense.bias',\n",
       " 'roberta.encoder.layer.6.attention.output.LayerNorm.weight',\n",
       " 'roberta.encoder.layer.6.attention.output.LayerNorm.bias',\n",
       " 'roberta.encoder.layer.6.intermediate.dense.weight',\n",
       " 'roberta.encoder.layer.6.intermediate.dense.bias',\n",
       " 'roberta.encoder.layer.6.output.dense.weight',\n",
       " 'roberta.encoder.layer.6.output.dense.bias',\n",
       " 'roberta.encoder.layer.6.output.LayerNorm.weight',\n",
       " 'roberta.encoder.layer.6.output.LayerNorm.bias',\n",
       " 'roberta.encoder.layer.7.attention.self.query.weight',\n",
       " 'roberta.encoder.layer.7.attention.self.query.bias',\n",
       " 'roberta.encoder.layer.7.attention.self.key.weight',\n",
       " 'roberta.encoder.layer.7.attention.self.key.bias',\n",
       " 'roberta.encoder.layer.7.attention.self.value.weight',\n",
       " 'roberta.encoder.layer.7.attention.self.value.bias',\n",
       " 'roberta.encoder.layer.7.attention.output.dense.weight',\n",
       " 'roberta.encoder.layer.7.attention.output.dense.bias',\n",
       " 'roberta.encoder.layer.7.attention.output.LayerNorm.weight',\n",
       " 'roberta.encoder.layer.7.attention.output.LayerNorm.bias',\n",
       " 'roberta.encoder.layer.7.intermediate.dense.weight',\n",
       " 'roberta.encoder.layer.7.intermediate.dense.bias',\n",
       " 'roberta.encoder.layer.7.output.dense.weight',\n",
       " 'roberta.encoder.layer.7.output.dense.bias',\n",
       " 'roberta.encoder.layer.7.output.LayerNorm.weight',\n",
       " 'roberta.encoder.layer.7.output.LayerNorm.bias',\n",
       " 'roberta.encoder.layer.8.attention.self.query.weight',\n",
       " 'roberta.encoder.layer.8.attention.self.query.bias',\n",
       " 'roberta.encoder.layer.8.attention.self.key.weight',\n",
       " 'roberta.encoder.layer.8.attention.self.key.bias',\n",
       " 'roberta.encoder.layer.8.attention.self.value.weight',\n",
       " 'roberta.encoder.layer.8.attention.self.value.bias',\n",
       " 'roberta.encoder.layer.8.attention.output.dense.weight',\n",
       " 'roberta.encoder.layer.8.attention.output.dense.bias',\n",
       " 'roberta.encoder.layer.8.attention.output.LayerNorm.weight',\n",
       " 'roberta.encoder.layer.8.attention.output.LayerNorm.bias',\n",
       " 'roberta.encoder.layer.8.intermediate.dense.weight',\n",
       " 'roberta.encoder.layer.8.intermediate.dense.bias',\n",
       " 'roberta.encoder.layer.8.output.dense.weight',\n",
       " 'roberta.encoder.layer.8.output.dense.bias',\n",
       " 'roberta.encoder.layer.8.output.LayerNorm.weight',\n",
       " 'roberta.encoder.layer.8.output.LayerNorm.bias',\n",
       " 'roberta.encoder.layer.9.attention.self.query.weight',\n",
       " 'roberta.encoder.layer.9.attention.self.query.bias',\n",
       " 'roberta.encoder.layer.9.attention.self.key.weight',\n",
       " 'roberta.encoder.layer.9.attention.self.key.bias',\n",
       " 'roberta.encoder.layer.9.attention.self.value.weight',\n",
       " 'roberta.encoder.layer.9.attention.self.value.bias',\n",
       " 'roberta.encoder.layer.9.attention.output.dense.weight',\n",
       " 'roberta.encoder.layer.9.attention.output.dense.bias',\n",
       " 'roberta.encoder.layer.9.attention.output.LayerNorm.weight',\n",
       " 'roberta.encoder.layer.9.attention.output.LayerNorm.bias',\n",
       " 'roberta.encoder.layer.9.intermediate.dense.weight',\n",
       " 'roberta.encoder.layer.9.intermediate.dense.bias',\n",
       " 'roberta.encoder.layer.9.output.dense.weight',\n",
       " 'roberta.encoder.layer.9.output.dense.bias',\n",
       " 'roberta.encoder.layer.9.output.LayerNorm.weight',\n",
       " 'roberta.encoder.layer.9.output.LayerNorm.bias',\n",
       " 'roberta.encoder.layer.10.attention.self.query.weight',\n",
       " 'roberta.encoder.layer.10.attention.self.query.bias',\n",
       " 'roberta.encoder.layer.10.attention.self.key.weight',\n",
       " 'roberta.encoder.layer.10.attention.self.key.bias',\n",
       " 'roberta.encoder.layer.10.attention.self.value.weight',\n",
       " 'roberta.encoder.layer.10.attention.self.value.bias',\n",
       " 'roberta.encoder.layer.10.attention.output.dense.weight',\n",
       " 'roberta.encoder.layer.10.attention.output.dense.bias',\n",
       " 'roberta.encoder.layer.10.attention.output.LayerNorm.weight',\n",
       " 'roberta.encoder.layer.10.attention.output.LayerNorm.bias',\n",
       " 'roberta.encoder.layer.10.intermediate.dense.weight',\n",
       " 'roberta.encoder.layer.10.intermediate.dense.bias',\n",
       " 'roberta.encoder.layer.10.output.dense.weight',\n",
       " 'roberta.encoder.layer.10.output.dense.bias',\n",
       " 'roberta.encoder.layer.10.output.LayerNorm.weight',\n",
       " 'roberta.encoder.layer.10.output.LayerNorm.bias',\n",
       " 'roberta.encoder.layer.11.attention.self.query.weight',\n",
       " 'roberta.encoder.layer.11.attention.self.query.bias',\n",
       " 'roberta.encoder.layer.11.attention.self.key.weight',\n",
       " 'roberta.encoder.layer.11.attention.self.key.bias',\n",
       " 'roberta.encoder.layer.11.attention.self.value.weight',\n",
       " 'roberta.encoder.layer.11.attention.self.value.bias',\n",
       " 'roberta.encoder.layer.11.attention.output.dense.weight',\n",
       " 'roberta.encoder.layer.11.attention.output.dense.bias',\n",
       " 'roberta.encoder.layer.11.attention.output.LayerNorm.weight',\n",
       " 'roberta.encoder.layer.11.attention.output.LayerNorm.bias',\n",
       " 'roberta.encoder.layer.11.intermediate.dense.weight',\n",
       " 'roberta.encoder.layer.11.intermediate.dense.bias',\n",
       " 'roberta.encoder.layer.11.output.dense.weight',\n",
       " 'roberta.encoder.layer.11.output.dense.bias',\n",
       " 'roberta.encoder.layer.11.output.LayerNorm.weight',\n",
       " 'roberta.encoder.layer.11.output.LayerNorm.bias',\n",
       " 'classifier.dense.weight',\n",
       " 'classifier.dense.bias',\n",
       " 'classifier.out_proj.weight',\n",
       " 'classifier.out_proj.bias']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(model.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, I arbitrarily choose the encoder output dense weights.\n",
    "\n",
    "### Use Dask\n",
    "\n",
    "Next, we pass each layer that we plan to work with (in this case, layers 0, 1, 2, and 3) to dask arrays. This will speed up our computing process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer0 = da.from_array(model[\"roberta.encoder.layer.0.output.dense.weight\"].detach().cpu().numpy()).flatten()\n",
    "layer1 = da.from_array(model[\"roberta.encoder.layer.1.output.dense.weight\"].detach().cpu().numpy()).flatten()\n",
    "layer2 = da.from_array(model[\"roberta.encoder.layer.2.output.dense.weight\"].detach().cpu().numpy()).flatten()\n",
    "layer3 = da.from_array(model[\"roberta.encoder.layer.3.output.dense.weight\"].detach().cpu().numpy()).flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decide on Partition\n",
    "\n",
    "Check the size of the layer to decide what partition should be used. A smaller partition will produce a more similar layer to the original, but will take more computing power."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2359296,)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer0.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9216.0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2359296/256"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the shape is divisible by 2, we can choose any partition size that is a power of 2. I'm going to try 256 since that will produce 9216 partitions of size 256, but will run in a reasonable amount of time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Force-Save Space\n",
    "\n",
    "Use of the following functions forces space to be saved. Every chunk will be replaced with a label. To see optional space saving, scroll down to the next section, where an MSE threshold can be specified."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use `save_space` from SSB\n",
    "\n",
    "This will produce a list of labels, one for each partition of the layer. Notice that `save_space` prints execution time updates throughout the process. I will repeat this for layers 1, 2, and 3, each time basing my label lists on layer 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunked keeper; execution time: 00:00:09\n",
      "chunked reducer; execution time: 00:00:19\n",
      "reducing list, iteration 0; execution time: 00:00:19\n",
      "reducing list, iteration 100; execution time: 00:00:27\n",
      "reducing list, iteration 200; execution time: 00:00:34\n",
      "reducing list, iteration 300; execution time: 00:00:41\n",
      "reducing list, iteration 400; execution time: 00:00:49\n",
      "reducing list, iteration 500; execution time: 00:00:56\n",
      "reducing list, iteration 600; execution time: 00:01:04\n",
      "reducing list, iteration 700; execution time: 00:01:11\n",
      "reducing list, iteration 800; execution time: 00:01:18\n",
      "reducing list, iteration 900; execution time: 00:01:26\n",
      "reducing list, iteration 1000; execution time: 00:01:33\n",
      "reducing list, iteration 1100; execution time: 00:01:40\n",
      "reducing list, iteration 1200; execution time: 00:01:48\n",
      "reducing list, iteration 1300; execution time: 00:01:55\n",
      "reducing list, iteration 1400; execution time: 00:02:02\n",
      "reducing list, iteration 1500; execution time: 00:02:10\n",
      "reducing list, iteration 1600; execution time: 00:02:17\n",
      "reducing list, iteration 1700; execution time: 00:02:24\n",
      "reducing list, iteration 1800; execution time: 00:02:32\n",
      "reducing list, iteration 1900; execution time: 00:02:39\n",
      "reducing list, iteration 2000; execution time: 00:02:46\n",
      "reducing list, iteration 2100; execution time: 00:02:54\n",
      "reducing list, iteration 2200; execution time: 00:03:01\n",
      "reducing list, iteration 2300; execution time: 00:03:08\n",
      "reducing list, iteration 2400; execution time: 00:03:16\n",
      "reducing list, iteration 2500; execution time: 00:03:23\n",
      "reducing list, iteration 2600; execution time: 00:03:30\n",
      "reducing list, iteration 2700; execution time: 00:03:38\n",
      "reducing list, iteration 2800; execution time: 00:03:45\n",
      "reducing list, iteration 2900; execution time: 00:03:52\n",
      "reducing list, iteration 3000; execution time: 00:04:00\n",
      "reducing list, iteration 3100; execution time: 00:04:07\n",
      "reducing list, iteration 3200; execution time: 00:04:14\n",
      "reducing list, iteration 3300; execution time: 00:04:22\n",
      "reducing list, iteration 3400; execution time: 00:04:29\n",
      "reducing list, iteration 3500; execution time: 00:04:36\n",
      "reducing list, iteration 3600; execution time: 00:04:44\n",
      "reducing list, iteration 3700; execution time: 00:04:51\n",
      "reducing list, iteration 3800; execution time: 00:04:58\n",
      "reducing list, iteration 3900; execution time: 00:05:06\n",
      "reducing list, iteration 4000; execution time: 00:05:13\n",
      "reducing list, iteration 4100; execution time: 00:05:20\n",
      "reducing list, iteration 4200; execution time: 00:05:27\n",
      "reducing list, iteration 4300; execution time: 00:05:35\n",
      "reducing list, iteration 4400; execution time: 00:05:42\n",
      "reducing list, iteration 4500; execution time: 00:05:49\n",
      "reducing list, iteration 4600; execution time: 00:05:57\n",
      "reducing list, iteration 4700; execution time: 00:06:04\n",
      "reducing list, iteration 4800; execution time: 00:06:11\n",
      "reducing list, iteration 4900; execution time: 00:06:19\n",
      "reducing list, iteration 5000; execution time: 00:06:26\n",
      "reducing list, iteration 5100; execution time: 00:06:33\n",
      "reducing list, iteration 5200; execution time: 00:06:41\n",
      "reducing list, iteration 5300; execution time: 00:06:48\n",
      "reducing list, iteration 5400; execution time: 00:06:55\n",
      "reducing list, iteration 5500; execution time: 00:07:03\n",
      "reducing list, iteration 5600; execution time: 00:07:10\n",
      "reducing list, iteration 5700; execution time: 00:07:17\n",
      "reducing list, iteration 5800; execution time: 00:07:25\n",
      "reducing list, iteration 5900; execution time: 00:07:32\n",
      "reducing list, iteration 6000; execution time: 00:07:39\n",
      "reducing list, iteration 6100; execution time: 00:07:47\n",
      "reducing list, iteration 6200; execution time: 00:07:54\n",
      "reducing list, iteration 6300; execution time: 00:08:01\n",
      "reducing list, iteration 6400; execution time: 00:08:09\n",
      "reducing list, iteration 6500; execution time: 00:08:16\n",
      "reducing list, iteration 6600; execution time: 00:08:23\n",
      "reducing list, iteration 6700; execution time: 00:08:31\n",
      "reducing list, iteration 6800; execution time: 00:08:38\n",
      "reducing list, iteration 6900; execution time: 00:08:45\n",
      "reducing list, iteration 7000; execution time: 00:08:53\n",
      "reducing list, iteration 7100; execution time: 00:09:00\n",
      "reducing list, iteration 7200; execution time: 00:09:07\n",
      "reducing list, iteration 7300; execution time: 00:09:15\n",
      "reducing list, iteration 7400; execution time: 00:09:22\n",
      "reducing list, iteration 7500; execution time: 00:09:29\n",
      "reducing list, iteration 7600; execution time: 00:09:37\n",
      "reducing list, iteration 7700; execution time: 00:09:44\n",
      "reducing list, iteration 7800; execution time: 00:09:51\n",
      "reducing list, iteration 7900; execution time: 00:09:59\n",
      "reducing list, iteration 8000; execution time: 00:10:06\n",
      "reducing list, iteration 8100; execution time: 00:10:13\n",
      "reducing list, iteration 8200; execution time: 00:10:21\n",
      "reducing list, iteration 8300; execution time: 00:10:28\n",
      "reducing list, iteration 8400; execution time: 00:10:35\n",
      "reducing list, iteration 8500; execution time: 00:10:43\n",
      "reducing list, iteration 8600; execution time: 00:10:50\n",
      "reducing list, iteration 8700; execution time: 00:10:57\n",
      "reducing list, iteration 8800; execution time: 00:11:05\n",
      "reducing list, iteration 8900; execution time: 00:11:12\n",
      "reducing list, iteration 9000; execution time: 00:11:19\n",
      "reducing list, iteration 9100; execution time: 00:11:27\n",
      "reducing list, iteration 9200; execution time: 00:11:34\n"
     ]
    }
   ],
   "source": [
    "layer1_labels = ssb.save_space(layer0, layer1, 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunked keeper; execution time: 00:00:10\n",
      "chunked reducer; execution time: 00:00:19\n",
      "reducing list, iteration 0; execution time: 00:00:19\n",
      "reducing list, iteration 100; execution time: 00:00:27\n",
      "reducing list, iteration 200; execution time: 00:00:34\n",
      "reducing list, iteration 300; execution time: 00:00:41\n",
      "reducing list, iteration 400; execution time: 00:00:49\n",
      "reducing list, iteration 500; execution time: 00:00:56\n",
      "reducing list, iteration 600; execution time: 00:01:04\n",
      "reducing list, iteration 700; execution time: 00:01:11\n",
      "reducing list, iteration 800; execution time: 00:01:18\n",
      "reducing list, iteration 900; execution time: 00:01:26\n",
      "reducing list, iteration 1000; execution time: 00:01:33\n",
      "reducing list, iteration 1100; execution time: 00:01:41\n",
      "reducing list, iteration 1200; execution time: 00:01:48\n",
      "reducing list, iteration 1300; execution time: 00:01:55\n",
      "reducing list, iteration 1400; execution time: 00:02:03\n",
      "reducing list, iteration 1500; execution time: 00:02:10\n",
      "reducing list, iteration 1600; execution time: 00:02:18\n",
      "reducing list, iteration 1700; execution time: 00:02:25\n",
      "reducing list, iteration 1800; execution time: 00:02:32\n",
      "reducing list, iteration 1900; execution time: 00:02:40\n",
      "reducing list, iteration 2000; execution time: 00:02:47\n",
      "reducing list, iteration 2100; execution time: 00:02:54\n",
      "reducing list, iteration 2200; execution time: 00:03:02\n",
      "reducing list, iteration 2300; execution time: 00:03:09\n",
      "reducing list, iteration 2400; execution time: 00:03:16\n",
      "reducing list, iteration 2500; execution time: 00:03:24\n",
      "reducing list, iteration 2600; execution time: 00:03:31\n",
      "reducing list, iteration 2700; execution time: 00:03:38\n",
      "reducing list, iteration 2800; execution time: 00:03:46\n",
      "reducing list, iteration 2900; execution time: 00:03:53\n",
      "reducing list, iteration 3000; execution time: 00:04:01\n",
      "reducing list, iteration 3100; execution time: 00:04:08\n",
      "reducing list, iteration 3200; execution time: 00:04:15\n",
      "reducing list, iteration 3300; execution time: 00:04:23\n",
      "reducing list, iteration 3400; execution time: 00:04:30\n",
      "reducing list, iteration 3500; execution time: 00:04:37\n",
      "reducing list, iteration 3600; execution time: 00:04:45\n",
      "reducing list, iteration 3700; execution time: 00:04:52\n",
      "reducing list, iteration 3800; execution time: 00:05:00\n",
      "reducing list, iteration 3900; execution time: 00:05:07\n",
      "reducing list, iteration 4000; execution time: 00:05:14\n",
      "reducing list, iteration 4100; execution time: 00:05:22\n",
      "reducing list, iteration 4200; execution time: 00:05:29\n",
      "reducing list, iteration 4300; execution time: 00:05:36\n",
      "reducing list, iteration 4400; execution time: 00:05:44\n",
      "reducing list, iteration 4500; execution time: 00:05:51\n",
      "reducing list, iteration 4600; execution time: 00:05:59\n",
      "reducing list, iteration 4700; execution time: 00:06:06\n",
      "reducing list, iteration 4800; execution time: 00:06:13\n",
      "reducing list, iteration 4900; execution time: 00:06:21\n",
      "reducing list, iteration 5000; execution time: 00:06:28\n",
      "reducing list, iteration 5100; execution time: 00:06:36\n",
      "reducing list, iteration 5200; execution time: 00:06:43\n",
      "reducing list, iteration 5300; execution time: 00:06:50\n",
      "reducing list, iteration 5400; execution time: 00:06:58\n",
      "reducing list, iteration 5500; execution time: 00:07:05\n",
      "reducing list, iteration 5600; execution time: 00:07:12\n",
      "reducing list, iteration 5700; execution time: 00:07:20\n",
      "reducing list, iteration 5800; execution time: 00:07:27\n",
      "reducing list, iteration 5900; execution time: 00:07:34\n",
      "reducing list, iteration 6000; execution time: 00:07:42\n",
      "reducing list, iteration 6100; execution time: 00:07:49\n",
      "reducing list, iteration 6200; execution time: 00:07:57\n",
      "reducing list, iteration 6300; execution time: 00:08:04\n",
      "reducing list, iteration 6400; execution time: 00:08:11\n",
      "reducing list, iteration 6500; execution time: 00:08:19\n",
      "reducing list, iteration 6600; execution time: 00:08:26\n",
      "reducing list, iteration 6700; execution time: 00:08:33\n",
      "reducing list, iteration 6800; execution time: 00:08:41\n",
      "reducing list, iteration 6900; execution time: 00:08:48\n",
      "reducing list, iteration 7000; execution time: 00:08:55\n",
      "reducing list, iteration 7100; execution time: 00:09:03\n",
      "reducing list, iteration 7200; execution time: 00:09:10\n",
      "reducing list, iteration 7300; execution time: 00:09:17\n",
      "reducing list, iteration 7400; execution time: 00:09:25\n",
      "reducing list, iteration 7500; execution time: 00:09:32\n",
      "reducing list, iteration 7600; execution time: 00:09:40\n",
      "reducing list, iteration 7700; execution time: 00:09:47\n",
      "reducing list, iteration 7800; execution time: 00:09:54\n",
      "reducing list, iteration 7900; execution time: 00:10:02\n",
      "reducing list, iteration 8000; execution time: 00:10:09\n",
      "reducing list, iteration 8100; execution time: 00:10:16\n",
      "reducing list, iteration 8200; execution time: 00:10:24\n",
      "reducing list, iteration 8300; execution time: 00:10:31\n",
      "reducing list, iteration 8400; execution time: 00:10:38\n",
      "reducing list, iteration 8500; execution time: 00:10:46\n",
      "reducing list, iteration 8600; execution time: 00:10:53\n",
      "reducing list, iteration 8700; execution time: 00:11:01\n",
      "reducing list, iteration 8800; execution time: 00:11:08\n",
      "reducing list, iteration 8900; execution time: 00:11:15\n",
      "reducing list, iteration 9000; execution time: 00:11:23\n",
      "reducing list, iteration 9100; execution time: 00:11:30\n",
      "reducing list, iteration 9200; execution time: 00:11:37\n"
     ]
    }
   ],
   "source": [
    "layer2_labels = ssb.save_space(layer0, layer2, 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunked keeper; execution time: 00:00:09\n",
      "chunked reducer; execution time: 00:00:19\n",
      "reducing list, iteration 0; execution time: 00:00:19\n",
      "reducing list, iteration 100; execution time: 00:00:27\n",
      "reducing list, iteration 200; execution time: 00:00:34\n",
      "reducing list, iteration 300; execution time: 00:00:41\n",
      "reducing list, iteration 400; execution time: 00:00:49\n",
      "reducing list, iteration 500; execution time: 00:00:56\n",
      "reducing list, iteration 600; execution time: 00:01:04\n",
      "reducing list, iteration 700; execution time: 00:01:11\n",
      "reducing list, iteration 800; execution time: 00:01:19\n",
      "reducing list, iteration 900; execution time: 00:01:26\n",
      "reducing list, iteration 1000; execution time: 00:01:34\n",
      "reducing list, iteration 1100; execution time: 00:01:41\n",
      "reducing list, iteration 1200; execution time: 00:01:48\n",
      "reducing list, iteration 1300; execution time: 00:01:56\n",
      "reducing list, iteration 1400; execution time: 00:02:03\n",
      "reducing list, iteration 1500; execution time: 00:02:11\n",
      "reducing list, iteration 1600; execution time: 00:02:18\n",
      "reducing list, iteration 1700; execution time: 00:02:26\n",
      "reducing list, iteration 1800; execution time: 00:02:33\n",
      "reducing list, iteration 1900; execution time: 00:02:41\n",
      "reducing list, iteration 2000; execution time: 00:02:48\n",
      "reducing list, iteration 2100; execution time: 00:02:56\n",
      "reducing list, iteration 2200; execution time: 00:03:03\n",
      "reducing list, iteration 2300; execution time: 00:03:10\n",
      "reducing list, iteration 2400; execution time: 00:03:18\n",
      "reducing list, iteration 2500; execution time: 00:03:25\n",
      "reducing list, iteration 2600; execution time: 00:03:33\n",
      "reducing list, iteration 2700; execution time: 00:03:40\n",
      "reducing list, iteration 2800; execution time: 00:03:48\n",
      "reducing list, iteration 2900; execution time: 00:03:55\n",
      "reducing list, iteration 3000; execution time: 00:04:02\n",
      "reducing list, iteration 3100; execution time: 00:04:10\n",
      "reducing list, iteration 3200; execution time: 00:04:17\n",
      "reducing list, iteration 3300; execution time: 00:04:25\n",
      "reducing list, iteration 3400; execution time: 00:04:32\n",
      "reducing list, iteration 3500; execution time: 00:04:40\n",
      "reducing list, iteration 3600; execution time: 00:04:47\n",
      "reducing list, iteration 3700; execution time: 00:04:55\n",
      "reducing list, iteration 3800; execution time: 00:05:02\n",
      "reducing list, iteration 3900; execution time: 00:05:10\n",
      "reducing list, iteration 4000; execution time: 00:05:17\n",
      "reducing list, iteration 4100; execution time: 00:05:24\n",
      "reducing list, iteration 4200; execution time: 00:05:32\n",
      "reducing list, iteration 4300; execution time: 00:05:39\n",
      "reducing list, iteration 4400; execution time: 00:05:47\n",
      "reducing list, iteration 4500; execution time: 00:05:54\n",
      "reducing list, iteration 4600; execution time: 00:06:02\n",
      "reducing list, iteration 4700; execution time: 00:06:09\n",
      "reducing list, iteration 4800; execution time: 00:06:16\n",
      "reducing list, iteration 4900; execution time: 00:06:24\n",
      "reducing list, iteration 5000; execution time: 00:06:31\n",
      "reducing list, iteration 5100; execution time: 00:06:39\n",
      "reducing list, iteration 5200; execution time: 00:06:46\n",
      "reducing list, iteration 5300; execution time: 00:06:53\n",
      "reducing list, iteration 5400; execution time: 00:07:01\n",
      "reducing list, iteration 5500; execution time: 00:07:08\n",
      "reducing list, iteration 5600; execution time: 00:07:16\n",
      "reducing list, iteration 5700; execution time: 00:07:23\n",
      "reducing list, iteration 5800; execution time: 00:07:30\n",
      "reducing list, iteration 5900; execution time: 00:07:38\n",
      "reducing list, iteration 6000; execution time: 00:07:45\n",
      "reducing list, iteration 6100; execution time: 00:07:53\n",
      "reducing list, iteration 6200; execution time: 00:08:00\n",
      "reducing list, iteration 6300; execution time: 00:08:08\n",
      "reducing list, iteration 6400; execution time: 00:08:15\n",
      "reducing list, iteration 6500; execution time: 00:08:22\n",
      "reducing list, iteration 6600; execution time: 00:08:30\n",
      "reducing list, iteration 6700; execution time: 00:08:37\n",
      "reducing list, iteration 6800; execution time: 00:08:45\n",
      "reducing list, iteration 6900; execution time: 00:08:52\n",
      "reducing list, iteration 7000; execution time: 00:09:00\n",
      "reducing list, iteration 7100; execution time: 00:09:07\n",
      "reducing list, iteration 7200; execution time: 00:09:14\n",
      "reducing list, iteration 7300; execution time: 00:09:22\n",
      "reducing list, iteration 7400; execution time: 00:09:29\n",
      "reducing list, iteration 7500; execution time: 00:09:37\n",
      "reducing list, iteration 7600; execution time: 00:09:44\n",
      "reducing list, iteration 7700; execution time: 00:09:52\n",
      "reducing list, iteration 7800; execution time: 00:09:59\n",
      "reducing list, iteration 7900; execution time: 00:10:07\n",
      "reducing list, iteration 8000; execution time: 00:10:14\n",
      "reducing list, iteration 8100; execution time: 00:10:21\n",
      "reducing list, iteration 8200; execution time: 00:10:29\n",
      "reducing list, iteration 8300; execution time: 00:10:36\n",
      "reducing list, iteration 8400; execution time: 00:10:44\n",
      "reducing list, iteration 8500; execution time: 00:10:51\n",
      "reducing list, iteration 8600; execution time: 00:10:59\n",
      "reducing list, iteration 8700; execution time: 00:11:06\n",
      "reducing list, iteration 8800; execution time: 00:11:14\n",
      "reducing list, iteration 8900; execution time: 00:11:21\n",
      "reducing list, iteration 9000; execution time: 00:11:28\n",
      "reducing list, iteration 9100; execution time: 00:11:36\n",
      "reducing list, iteration 9200; execution time: 00:11:43\n"
     ]
    }
   ],
   "source": [
    "layer3_labels = ssb.save_space(layer0, layer3, 256)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate Reduced-Space Model\n",
    "\n",
    "Create a copy of the model that we'll now change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_resize = torch.load('pretrained_models/Giannipinelli-xlm-roberta-base-finetuned-marc-en/pytorch_model.bin', map_location='cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the size of the original model layers. We'll store these sizes in a new parameter so that later, we can regenerate layers with the correct dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "shape1 = np.array(model_resize['roberta.encoder.layer.1.output.dense.weight'].size())\n",
    "shape2 = np.array(model_resize['roberta.encoder.layer.2.output.dense.weight'].size())\n",
    "shape3 = np.array(model_resize['roberta.encoder.layer.3.output.dense.weight'].size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now empty the three original model layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_resize['roberta.encoder.layer.1.output.dense.weight'] = None\n",
    "model_resize['roberta.encoder.layer.2.output.dense.weight'] = None\n",
    "model_resize['roberta.encoder.layer.3.output.dense.weight'] = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, pass the label lists to the model layer locations, and pass the stored shapes to another parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_resize['roberta.encoder.layer.1.output.dense.labels'] = layer1_labels\n",
    "model_resize['roberta.encoder.layer.2.output.dense.labels'] = layer2_labels\n",
    "model_resize['roberta.encoder.layer.3.output.dense.labels'] = layer3_labels\n",
    "model_resize['roberta.encoder.layers.output.dense.sizes'] = [shape1, shape2, shape3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we're ready to save the model. I save it in the same folder with a different name, so I have the original and the resized in the same place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model_resize, \"/scratch/rg5xm/pretrained_models/Giannipinelli-xlm-roberta-base-finetuned-marc-en/pytorch_model_resized.bin\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resized model file only takes up 1.01 GB! We have successfully saved space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optionally-Save Space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use `save_space_opt` from SSB\n",
    "\n",
    "This will produce a list of labels and arrays, one for each partition of the layer. If the lowest MSE for a chunk is above the MSE threshold specified, the chunk will not be replaced with a label. Like `save_space`, it prints execution time updates throughout the process. I will repeat this for layers 1, 2, and 3, each time basing my label lists on layer 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Use `get_avg_mse`\n",
    "\n",
    "Use this to decide on an mse to pick. In the example below, I picked size of 256 and MSEs for each layer that are slightly lower than the average MSE for that layer. We'll see how different it is from the force-saved space!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0037366975"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_256_1 = ssb.get_avg_mse(layer0, layer1, 256, 100)\n",
    "avg_256_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0041618789"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_256_2 = ssb.get_avg_mse(layer0, layer2, 256, 100)\n",
    "avg_256_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.005202455"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_256_3 = ssb.get_avg_mse(layer0, layer3, 256, 100)\n",
    "avg_256_3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Generate Layer Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunked keeper; execution time: 00:00:09\n",
      "chunked reducer; execution time: 00:00:18\n",
      "reducing list, iteration 0; execution time: 00:00:18\n",
      "reducing list, iteration 100; execution time: 00:00:26\n",
      "reducing list, iteration 200; execution time: 00:00:34\n",
      "reducing list, iteration 300; execution time: 00:00:43\n",
      "reducing list, iteration 400; execution time: 00:00:51\n",
      "reducing list, iteration 500; execution time: 00:00:59\n",
      "reducing list, iteration 600; execution time: 00:01:07\n",
      "reducing list, iteration 700; execution time: 00:01:15\n",
      "reducing list, iteration 800; execution time: 00:01:23\n",
      "reducing list, iteration 900; execution time: 00:01:31\n",
      "reducing list, iteration 1000; execution time: 00:01:39\n",
      "reducing list, iteration 1100; execution time: 00:01:47\n",
      "reducing list, iteration 1200; execution time: 00:01:55\n",
      "reducing list, iteration 1300; execution time: 00:02:03\n",
      "reducing list, iteration 1400; execution time: 00:02:11\n",
      "reducing list, iteration 1500; execution time: 00:02:19\n",
      "reducing list, iteration 1600; execution time: 00:02:27\n",
      "reducing list, iteration 1700; execution time: 00:02:35\n",
      "reducing list, iteration 1800; execution time: 00:02:43\n",
      "reducing list, iteration 1900; execution time: 00:02:51\n",
      "reducing list, iteration 2000; execution time: 00:02:59\n",
      "reducing list, iteration 2100; execution time: 00:03:07\n",
      "reducing list, iteration 2200; execution time: 00:03:15\n",
      "reducing list, iteration 2300; execution time: 00:03:23\n",
      "reducing list, iteration 2400; execution time: 00:03:31\n",
      "reducing list, iteration 2500; execution time: 00:03:39\n",
      "reducing list, iteration 2600; execution time: 00:03:47\n",
      "reducing list, iteration 2700; execution time: 00:03:55\n",
      "reducing list, iteration 2800; execution time: 00:04:03\n",
      "reducing list, iteration 2900; execution time: 00:04:11\n",
      "reducing list, iteration 3000; execution time: 00:04:19\n",
      "reducing list, iteration 3100; execution time: 00:04:27\n",
      "reducing list, iteration 3200; execution time: 00:04:35\n",
      "reducing list, iteration 3300; execution time: 00:04:43\n",
      "reducing list, iteration 3400; execution time: 00:04:51\n",
      "reducing list, iteration 3500; execution time: 00:05:00\n",
      "reducing list, iteration 3600; execution time: 00:05:08\n",
      "reducing list, iteration 3700; execution time: 00:05:16\n",
      "reducing list, iteration 3800; execution time: 00:05:24\n",
      "reducing list, iteration 3900; execution time: 00:05:32\n",
      "reducing list, iteration 4000; execution time: 00:05:40\n",
      "reducing list, iteration 4100; execution time: 00:05:48\n",
      "reducing list, iteration 4200; execution time: 00:05:56\n",
      "reducing list, iteration 4300; execution time: 00:06:04\n",
      "reducing list, iteration 4400; execution time: 00:06:12\n",
      "reducing list, iteration 4500; execution time: 00:06:20\n",
      "reducing list, iteration 4600; execution time: 00:06:28\n",
      "reducing list, iteration 4700; execution time: 00:06:36\n",
      "reducing list, iteration 4800; execution time: 00:06:44\n",
      "reducing list, iteration 4900; execution time: 00:06:52\n",
      "reducing list, iteration 5000; execution time: 00:07:00\n",
      "reducing list, iteration 5100; execution time: 00:07:08\n",
      "reducing list, iteration 5200; execution time: 00:07:16\n",
      "reducing list, iteration 5300; execution time: 00:07:24\n",
      "reducing list, iteration 5400; execution time: 00:07:32\n",
      "reducing list, iteration 5500; execution time: 00:07:40\n",
      "reducing list, iteration 5600; execution time: 00:07:48\n",
      "reducing list, iteration 5700; execution time: 00:07:56\n",
      "reducing list, iteration 5800; execution time: 00:08:04\n",
      "reducing list, iteration 5900; execution time: 00:08:12\n",
      "reducing list, iteration 6000; execution time: 00:08:20\n",
      "reducing list, iteration 6100; execution time: 00:08:28\n",
      "reducing list, iteration 6200; execution time: 00:08:36\n",
      "reducing list, iteration 6300; execution time: 00:08:44\n",
      "reducing list, iteration 6400; execution time: 00:08:52\n",
      "reducing list, iteration 6500; execution time: 00:09:00\n",
      "reducing list, iteration 6600; execution time: 00:09:08\n",
      "reducing list, iteration 6700; execution time: 00:09:16\n",
      "reducing list, iteration 6800; execution time: 00:09:25\n",
      "reducing list, iteration 6900; execution time: 00:09:33\n",
      "reducing list, iteration 7000; execution time: 00:09:41\n",
      "reducing list, iteration 7100; execution time: 00:09:48\n",
      "reducing list, iteration 7200; execution time: 00:09:57\n",
      "reducing list, iteration 7300; execution time: 00:10:04\n",
      "reducing list, iteration 7400; execution time: 00:10:12\n",
      "reducing list, iteration 7500; execution time: 00:10:20\n",
      "reducing list, iteration 7600; execution time: 00:10:29\n",
      "reducing list, iteration 7700; execution time: 00:10:37\n",
      "reducing list, iteration 7800; execution time: 00:10:45\n",
      "reducing list, iteration 7900; execution time: 00:10:53\n",
      "reducing list, iteration 8000; execution time: 00:11:01\n",
      "reducing list, iteration 8100; execution time: 00:11:09\n",
      "reducing list, iteration 8200; execution time: 00:11:17\n",
      "reducing list, iteration 8300; execution time: 00:11:25\n",
      "reducing list, iteration 8400; execution time: 00:11:33\n",
      "reducing list, iteration 8500; execution time: 00:11:41\n",
      "reducing list, iteration 8600; execution time: 00:11:49\n",
      "reducing list, iteration 8700; execution time: 00:11:57\n",
      "reducing list, iteration 8800; execution time: 00:12:05\n",
      "reducing list, iteration 8900; execution time: 00:12:13\n",
      "reducing list, iteration 9000; execution time: 00:12:21\n",
      "reducing list, iteration 9100; execution time: 00:12:29\n",
      "reducing list, iteration 9200; execution time: 00:12:37\n"
     ]
    }
   ],
   "source": [
    "layer1_labels_opt = ssb.save_space_opt(layer0, layer1, 256, 0.0037)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunked keeper; execution time: 00:00:09\n",
      "chunked reducer; execution time: 00:00:18\n",
      "reducing list, iteration 0; execution time: 00:00:18\n",
      "reducing list, iteration 100; execution time: 00:00:26\n",
      "reducing list, iteration 200; execution time: 00:00:35\n",
      "reducing list, iteration 300; execution time: 00:00:43\n",
      "reducing list, iteration 400; execution time: 00:00:51\n",
      "reducing list, iteration 500; execution time: 00:00:59\n",
      "reducing list, iteration 600; execution time: 00:01:08\n",
      "reducing list, iteration 700; execution time: 00:01:16\n",
      "reducing list, iteration 800; execution time: 00:01:24\n",
      "reducing list, iteration 900; execution time: 00:01:32\n",
      "reducing list, iteration 1000; execution time: 00:01:40\n",
      "reducing list, iteration 1100; execution time: 00:01:48\n",
      "reducing list, iteration 1200; execution time: 00:01:56\n",
      "reducing list, iteration 1300; execution time: 00:02:04\n",
      "reducing list, iteration 1400; execution time: 00:02:12\n",
      "reducing list, iteration 1500; execution time: 00:02:20\n",
      "reducing list, iteration 1600; execution time: 00:02:28\n",
      "reducing list, iteration 1700; execution time: 00:02:36\n",
      "reducing list, iteration 1800; execution time: 00:02:44\n",
      "reducing list, iteration 1900; execution time: 00:02:52\n",
      "reducing list, iteration 2000; execution time: 00:03:00\n",
      "reducing list, iteration 2100; execution time: 00:03:08\n",
      "reducing list, iteration 2200; execution time: 00:03:16\n",
      "reducing list, iteration 2300; execution time: 00:03:25\n",
      "reducing list, iteration 2400; execution time: 00:03:33\n",
      "reducing list, iteration 2500; execution time: 00:03:41\n",
      "reducing list, iteration 2600; execution time: 00:03:49\n",
      "reducing list, iteration 2700; execution time: 00:03:57\n",
      "reducing list, iteration 2800; execution time: 00:04:05\n",
      "reducing list, iteration 2900; execution time: 00:04:13\n",
      "reducing list, iteration 3000; execution time: 00:04:21\n",
      "reducing list, iteration 3100; execution time: 00:04:29\n",
      "reducing list, iteration 3200; execution time: 00:04:37\n",
      "reducing list, iteration 3300; execution time: 00:04:45\n",
      "reducing list, iteration 3400; execution time: 00:04:53\n",
      "reducing list, iteration 3500; execution time: 00:05:01\n",
      "reducing list, iteration 3600; execution time: 00:05:09\n",
      "reducing list, iteration 3700; execution time: 00:05:17\n",
      "reducing list, iteration 3800; execution time: 00:05:25\n",
      "reducing list, iteration 3900; execution time: 00:05:33\n",
      "reducing list, iteration 4000; execution time: 00:05:41\n",
      "reducing list, iteration 4100; execution time: 00:05:49\n",
      "reducing list, iteration 4200; execution time: 00:05:57\n",
      "reducing list, iteration 4300; execution time: 00:06:05\n",
      "reducing list, iteration 4400; execution time: 00:06:13\n",
      "reducing list, iteration 4500; execution time: 00:06:21\n",
      "reducing list, iteration 4600; execution time: 00:06:29\n",
      "reducing list, iteration 4700; execution time: 00:06:37\n",
      "reducing list, iteration 4800; execution time: 00:06:45\n",
      "reducing list, iteration 4900; execution time: 00:06:53\n",
      "reducing list, iteration 5000; execution time: 00:07:01\n",
      "reducing list, iteration 5100; execution time: 00:07:09\n",
      "reducing list, iteration 5200; execution time: 00:07:17\n",
      "reducing list, iteration 5300; execution time: 00:07:25\n",
      "reducing list, iteration 5400; execution time: 00:07:33\n",
      "reducing list, iteration 5500; execution time: 00:07:41\n",
      "reducing list, iteration 5600; execution time: 00:07:49\n",
      "reducing list, iteration 5700; execution time: 00:07:57\n",
      "reducing list, iteration 5800; execution time: 00:08:05\n",
      "reducing list, iteration 5900; execution time: 00:08:13\n",
      "reducing list, iteration 6000; execution time: 00:08:21\n",
      "reducing list, iteration 6100; execution time: 00:08:29\n",
      "reducing list, iteration 6200; execution time: 00:08:37\n",
      "reducing list, iteration 6300; execution time: 00:08:45\n",
      "reducing list, iteration 6400; execution time: 00:08:53\n",
      "reducing list, iteration 6500; execution time: 00:09:01\n",
      "reducing list, iteration 6600; execution time: 00:09:09\n",
      "reducing list, iteration 6700; execution time: 00:09:17\n",
      "reducing list, iteration 6800; execution time: 00:09:25\n",
      "reducing list, iteration 6900; execution time: 00:09:33\n",
      "reducing list, iteration 7000; execution time: 00:09:41\n",
      "reducing list, iteration 7100; execution time: 00:09:49\n",
      "reducing list, iteration 7200; execution time: 00:09:57\n",
      "reducing list, iteration 7300; execution time: 00:10:05\n",
      "reducing list, iteration 7400; execution time: 00:10:13\n",
      "reducing list, iteration 7500; execution time: 00:10:21\n",
      "reducing list, iteration 7600; execution time: 00:10:29\n",
      "reducing list, iteration 7700; execution time: 00:10:37\n",
      "reducing list, iteration 7800; execution time: 00:10:45\n",
      "reducing list, iteration 7900; execution time: 00:10:53\n",
      "reducing list, iteration 8000; execution time: 00:11:01\n",
      "reducing list, iteration 8100; execution time: 00:11:09\n",
      "reducing list, iteration 8200; execution time: 00:11:17\n",
      "reducing list, iteration 8300; execution time: 00:11:25\n",
      "reducing list, iteration 8400; execution time: 00:11:33\n",
      "reducing list, iteration 8500; execution time: 00:11:41\n",
      "reducing list, iteration 8600; execution time: 00:11:49\n",
      "reducing list, iteration 8700; execution time: 00:11:57\n",
      "reducing list, iteration 8800; execution time: 00:12:05\n",
      "reducing list, iteration 8900; execution time: 00:12:13\n",
      "reducing list, iteration 9000; execution time: 00:12:21\n",
      "reducing list, iteration 9100; execution time: 00:12:29\n",
      "reducing list, iteration 9200; execution time: 00:12:37\n"
     ]
    }
   ],
   "source": [
    "layer2_labels_opt = ssb.save_space_opt(layer0, layer2, 256, 0.004)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunked keeper; execution time: 00:00:09\n",
      "chunked reducer; execution time: 00:00:18\n",
      "reducing list, iteration 0; execution time: 00:00:18\n",
      "reducing list, iteration 100; execution time: 00:00:26\n",
      "reducing list, iteration 200; execution time: 00:00:34\n",
      "reducing list, iteration 300; execution time: 00:00:42\n",
      "reducing list, iteration 400; execution time: 00:00:50\n",
      "reducing list, iteration 500; execution time: 00:00:58\n",
      "reducing list, iteration 600; execution time: 00:01:07\n",
      "reducing list, iteration 700; execution time: 00:01:15\n",
      "reducing list, iteration 800; execution time: 00:01:23\n",
      "reducing list, iteration 900; execution time: 00:01:31\n",
      "reducing list, iteration 1000; execution time: 00:01:39\n",
      "reducing list, iteration 1100; execution time: 00:01:47\n",
      "reducing list, iteration 1200; execution time: 00:01:55\n",
      "reducing list, iteration 1300; execution time: 00:02:03\n",
      "reducing list, iteration 1400; execution time: 00:02:11\n",
      "reducing list, iteration 1500; execution time: 00:02:19\n",
      "reducing list, iteration 1600; execution time: 00:02:27\n",
      "reducing list, iteration 1700; execution time: 00:02:35\n",
      "reducing list, iteration 1800; execution time: 00:02:43\n",
      "reducing list, iteration 1900; execution time: 00:02:51\n",
      "reducing list, iteration 2000; execution time: 00:03:00\n",
      "reducing list, iteration 2100; execution time: 00:03:08\n",
      "reducing list, iteration 2200; execution time: 00:03:16\n",
      "reducing list, iteration 2300; execution time: 00:03:24\n",
      "reducing list, iteration 2400; execution time: 00:03:31\n",
      "reducing list, iteration 2500; execution time: 00:03:39\n",
      "reducing list, iteration 2600; execution time: 00:03:47\n",
      "reducing list, iteration 2700; execution time: 00:03:55\n",
      "reducing list, iteration 2800; execution time: 00:04:03\n",
      "reducing list, iteration 2900; execution time: 00:04:11\n",
      "reducing list, iteration 3000; execution time: 00:04:19\n",
      "reducing list, iteration 3100; execution time: 00:04:27\n",
      "reducing list, iteration 3200; execution time: 00:04:35\n",
      "reducing list, iteration 3300; execution time: 00:04:42\n",
      "reducing list, iteration 3400; execution time: 00:04:50\n",
      "reducing list, iteration 3500; execution time: 00:04:58\n",
      "reducing list, iteration 3600; execution time: 00:05:06\n",
      "reducing list, iteration 3700; execution time: 00:05:14\n",
      "reducing list, iteration 3800; execution time: 00:05:22\n",
      "reducing list, iteration 3900; execution time: 00:05:30\n",
      "reducing list, iteration 4000; execution time: 00:05:38\n",
      "reducing list, iteration 4100; execution time: 00:05:46\n",
      "reducing list, iteration 4200; execution time: 00:05:53\n",
      "reducing list, iteration 4300; execution time: 00:06:01\n",
      "reducing list, iteration 4400; execution time: 00:06:09\n",
      "reducing list, iteration 4500; execution time: 00:06:17\n",
      "reducing list, iteration 4600; execution time: 00:06:25\n",
      "reducing list, iteration 4700; execution time: 00:06:33\n",
      "reducing list, iteration 4800; execution time: 00:06:41\n",
      "reducing list, iteration 4900; execution time: 00:06:49\n",
      "reducing list, iteration 5000; execution time: 00:06:57\n",
      "reducing list, iteration 5100; execution time: 00:07:05\n",
      "reducing list, iteration 5200; execution time: 00:07:13\n",
      "reducing list, iteration 5300; execution time: 00:07:21\n",
      "reducing list, iteration 5400; execution time: 00:07:29\n",
      "reducing list, iteration 5500; execution time: 00:07:36\n",
      "reducing list, iteration 5600; execution time: 00:07:44\n",
      "reducing list, iteration 5700; execution time: 00:07:52\n",
      "reducing list, iteration 5800; execution time: 00:08:00\n",
      "reducing list, iteration 5900; execution time: 00:08:08\n",
      "reducing list, iteration 6000; execution time: 00:08:16\n",
      "reducing list, iteration 6100; execution time: 00:08:24\n",
      "reducing list, iteration 6200; execution time: 00:08:32\n",
      "reducing list, iteration 6300; execution time: 00:08:40\n",
      "reducing list, iteration 6400; execution time: 00:08:48\n",
      "reducing list, iteration 6500; execution time: 00:08:56\n",
      "reducing list, iteration 6600; execution time: 00:09:04\n",
      "reducing list, iteration 6700; execution time: 00:09:12\n",
      "reducing list, iteration 6800; execution time: 00:09:19\n",
      "reducing list, iteration 6900; execution time: 00:09:27\n",
      "reducing list, iteration 7000; execution time: 00:09:35\n",
      "reducing list, iteration 7100; execution time: 00:09:43\n",
      "reducing list, iteration 7200; execution time: 00:09:51\n",
      "reducing list, iteration 7300; execution time: 00:09:59\n",
      "reducing list, iteration 7400; execution time: 00:10:07\n",
      "reducing list, iteration 7500; execution time: 00:10:15\n",
      "reducing list, iteration 7600; execution time: 00:10:23\n",
      "reducing list, iteration 7700; execution time: 00:10:31\n",
      "reducing list, iteration 7800; execution time: 00:10:39\n",
      "reducing list, iteration 7900; execution time: 00:10:47\n",
      "reducing list, iteration 8000; execution time: 00:10:55\n",
      "reducing list, iteration 8100; execution time: 00:11:02\n",
      "reducing list, iteration 8200; execution time: 00:11:10\n",
      "reducing list, iteration 8300; execution time: 00:11:18\n",
      "reducing list, iteration 8400; execution time: 00:11:26\n",
      "reducing list, iteration 8500; execution time: 00:11:34\n",
      "reducing list, iteration 8600; execution time: 00:11:42\n",
      "reducing list, iteration 8700; execution time: 00:11:50\n",
      "reducing list, iteration 8800; execution time: 00:11:58\n",
      "reducing list, iteration 8900; execution time: 00:12:06\n",
      "reducing list, iteration 9000; execution time: 00:12:14\n",
      "reducing list, iteration 9100; execution time: 00:12:22\n",
      "reducing list, iteration 9200; execution time: 00:12:30\n"
     ]
    }
   ],
   "source": [
    "layer3_labels_opt = ssb.save_space_opt(layer0, layer3, 256, 0.005)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate Reduced-Space Model\n",
    "\n",
    "Create a copy of the model that we'll now change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_resize_opt = torch.load('pretrained_models/Giannipinelli-xlm-roberta-base-finetuned-marc-en/pytorch_model.bin', map_location='cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the size of the original model layers. We'll store these sizes in a new parameter so that later, we can regenerate layers with the correct dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "shape1 = np.array(model_resize_opt['roberta.encoder.layer.1.output.dense.weight'].size())\n",
    "shape2 = np.array(model_resize_opt['roberta.encoder.layer.2.output.dense.weight'].size())\n",
    "shape3 = np.array(model_resize_opt['roberta.encoder.layer.3.output.dense.weight'].size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now empty the three original model layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_resize_opt['roberta.encoder.layer.1.output.dense.weight'] = None\n",
    "model_resize_opt['roberta.encoder.layer.2.output.dense.weight'] = None\n",
    "model_resize_opt['roberta.encoder.layer.3.output.dense.weight'] = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, pass the label lists to the model layer locations, and pass the stored shapes to another parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'layer1_labels_opt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-ec01c2cbb351>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel_resize_opt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'roberta.encoder.layer.1.output.dense.labels'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer1_labels_opt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mmodel_resize_opt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'roberta.encoder.layer.2.output.dense.labels'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer2_labels_opt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmodel_resize_opt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'roberta.encoder.layer.3.output.dense.labels'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer3_labels_opt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mmodel_resize_opt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'roberta.encoder.layers.output.dense.sizes'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mshape1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'layer1_labels_opt' is not defined"
     ]
    }
   ],
   "source": [
    "model_resize_opt['roberta.encoder.layer.1.output.dense.labels'] = layer1_labels_opt\n",
    "model_resize_opt['roberta.encoder.layer.2.output.dense.labels'] = layer2_labels_opt\n",
    "model_resize_opt['roberta.encoder.layer.3.output.dense.labels'] = layer3_labels_opt\n",
    "model_resize_opt['roberta.encoder.layers.output.dense.sizes'] = [shape1, shape2, shape3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we're ready to save the model. I save it in the same folder with a different name, so I have the original and the resized in the same place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model_resize_opt, \"/scratch/rg5xm/pretrained_models/Giannipinelli-xlm-roberta-base-finetuned-marc-en/pytorch_model_resized_opt.bin\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resized model file only takes up **1.01** GB! We have successfully saved space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating Force-Saved Model from Reduced Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This portion of the instructions demonstrate how to reconstruct the model from the reduced-size version that we just created."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Model\n",
    "\n",
    "Load the reduced size model like so. If working in a new notebook, make sure to import the packages at the top of this page, particularly SpaceSaverBERT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_reduced = torch.load('pretrained_models/Giannipinelli-xlm-roberta-base-finetuned-marc-en/pytorch_model_resized.bin', map_location='cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Re-Create Layers with `generate_list`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunked keeper; execution time: 00:00:09\n",
      "layer generated; execution time: 00:00:09\n",
      "layer flattened; execution time: 00:00:09\n"
     ]
    }
   ],
   "source": [
    "new_layer1 = ssb.generate_list(model_reduced[\"roberta.encoder.layer.0.output.dense.weight\"],\n",
    "                               model_reduced[\"roberta.encoder.layer.1.output.dense.labels\"], 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunked keeper; execution time: 00:00:09\n",
      "layer generated; execution time: 00:00:09\n",
      "layer flattened; execution time: 00:00:09\n"
     ]
    }
   ],
   "source": [
    "new_layer2 = ssb.generate_list(model_reduced[\"roberta.encoder.layer.0.output.dense.weight\"],\n",
    "                               model_reduced[\"roberta.encoder.layer.2.output.dense.labels\"], 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunked keeper; execution time: 00:00:09\n",
      "layer generated; execution time: 00:00:09\n",
      "layer flattened; execution time: 00:00:09\n"
     ]
    }
   ],
   "source": [
    "new_layer3 = ssb.generate_list(model_reduced[\"roberta.encoder.layer.0.output.dense.weight\"], \n",
    "                               model_reduced[\"roberta.encoder.layer.3.output.dense.labels\"], 256)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pass New Layers to Model for Use\n",
    "\n",
    "To be able to implement this model, we lastly need to pass these newly generated layers to the old layer locations. Currently, those locations store the chunk labels, not an entire parameter layer. This is where our parameter storing the original shapes of the layers comes into play, as we need to convert our generated layers from lists to PyTorch tensors. We index into that parameter to get the appropriate size for each layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_layer1_tensor = ssb.list_to_tensor(new_layer1, model_reduced['roberta.encoder.layers.output.dense.sizes'][0])\n",
    "new_layer2_tensor = ssb.list_to_tensor(new_layer2, model_reduced['roberta.encoder.layers.output.dense.sizes'][1])\n",
    "new_layer3_tensor = ssb.list_to_tensor(new_layer3, model_reduced['roberta.encoder.layers.output.dense.sizes'][2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now assign the tensors to the layer names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_reduced['roberta.encoder.layer.1.output.dense.weight'] = new_layer1_tensor\n",
    "model_reduced['roberta.encoder.layer.2.output.dense.weight'] = new_layer2_tensor\n",
    "model_reduced['roberta.encoder.layer.3.output.dense.weight'] = new_layer3_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reduced model is now ready for use! It can be implemented in this jupyter notebook or saved and implemented in another notebook. If saving it, do it like so. I'm saving it in the same location as the other models, and calling it 'pytorch_model_mod.bin' since this is not reduced in space, but modified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model_reduced, \"/scratch/rg5xm/pretrained_models/Giannipinelli-xlm-roberta-base-finetuned-marc-en/pytorch_model_mod.bin\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Optionally-Saved Model from Optionally-Reduced Model\n",
    "\n",
    "This portion of the instructions demonstrate how to reconstruct the model from the reduced-size version that we just created."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Model\n",
    "\n",
    "Load the reduced size optionally-saved model like so. If working in a new notebook, make sure to import the packages at the top of this page, particularly SpaceSaverBERT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_reduced_opt = torch.load('pretrained_models/Giannipinelli-xlm-roberta-base-finetuned-marc-en/pytorch_model_resized_opt.bin', map_location='cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Re-Create Layers with `generate_list_opt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunked keeper; execution time: 00:00:09\n",
      "layer generated; execution time: 00:00:09\n",
      "layer flattened; execution time: 00:00:09\n"
     ]
    }
   ],
   "source": [
    "new_layer1_opt = ssb.generate_list_opt(model_reduced_opt[\"roberta.encoder.layer.0.output.dense.weight\"],\n",
    "                                   model_reduced_opt[\"roberta.encoder.layer.1.output.dense.labels\"], 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunked keeper; execution time: 00:00:09\n",
      "layer generated; execution time: 00:00:09\n",
      "layer flattened; execution time: 00:00:09\n"
     ]
    }
   ],
   "source": [
    "new_layer2_opt = ssb.generate_list_opt(model_reduced_opt[\"roberta.encoder.layer.0.output.dense.weight\"],\n",
    "                                   model_reduced_opt[\"roberta.encoder.layer.2.output.dense.labels\"], 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunked keeper; execution time: 00:00:09\n",
      "layer generated; execution time: 00:00:09\n",
      "layer flattened; execution time: 00:00:09\n"
     ]
    }
   ],
   "source": [
    "new_layer3_opt = ssb.generate_list_opt(model_reduced_opt[\"roberta.encoder.layer.0.output.dense.weight\"],\n",
    "                                   model_reduced_opt[\"roberta.encoder.layer.3.output.dense.labels\"], 256)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pass New Layers to Model for Use\n",
    "\n",
    "To be able to implement this model, we lastly need to pass these newly generated layers to the old layer locations. Currently, those locations store the chunk labels, not an entire parameter layer. This is where our parameter storing the original shapes of the layers comes into play, as we need to convert our generated layers from lists to PyTorch tensors. We index into that parameter to get the appropriate size for each layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_layer1_tensor_opt = ssb.list_to_tensor(new_layer1_opt, model_reduced_opt['roberta.encoder.layers.output.dense.sizes'][0])\n",
    "new_layer2_tensor_opt = ssb.list_to_tensor(new_layer2_opt, model_reduced_opt['roberta.encoder.layers.output.dense.sizes'][1])\n",
    "new_layer3_tensor_opt = ssb.list_to_tensor(new_layer3_opt, model_reduced_opt['roberta.encoder.layers.output.dense.sizes'][2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now assign the tensors to the layer names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_reduced_opt['roberta.encoder.layer.1.output.dense.weight'] = new_layer1_tensor_opt\n",
    "model_reduced_opt['roberta.encoder.layer.2.output.dense.weight'] = new_layer2_tensor_opt\n",
    "model_reduced_opt['roberta.encoder.layer.3.output.dense.weight'] = new_layer3_tensor_opt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reduced model is now ready for use! It can be implemented in this jupyter notebook or saved and implemented in another notebook. If saving it, do it like so. I'm saving it in the same location as the other models, and calling it 'pytorch_model_mod.bin' since this is not reduced in space, but modified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model_reduced_opt, \"/scratch/rg5xm/pretrained_models/Giannipinelli-xlm-roberta-base-finetuned-marc-en/pytorch_model_mod_opt.bin\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
